{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import logging\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "from src.lda import lda_funcs\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "handler = logging.FileHandler('../models/training_output.log')\n",
    "\n",
    "# The handler above is somthing I needed with respect to logging.\n",
    "# Gensim performs various calculations while training the LDA model that I am using, but the only way to see them\n",
    "# is in the logging outputs.\n",
    "# Specifically, I need to capture the perplexity values during training to verify that perplexity is decreasing.\n",
    "# This metric is needed to compare models and to do hyperparameter tuning. \n",
    "\n",
    "\n",
    "# The following blog post was helpful to me in figure out how to make the log handler I needed.\n",
    "# https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/reviews.csv', index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get review counts for each product\n",
    "review_counts = df['ProductId'].value_counts().sort_values()\n",
    "review_counts.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a product group to work with - the 10 products with the largest number of reviews\n",
    "top_ten = review_counts.tail(10).index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Grid Search & Tuning\n",
    "\n",
    "For each of the input text types (vanilla, coded, and valence coded), first run an initial grid search with 50 or 80 training passes and either 6, 10, or 12 topics.\n",
    "\n",
    "NOTE: grid search can take hours to run fully - don't run the below cells unless you actually want to perform the searching and tuning. The csv results files can simply be loaded in where noted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "# load in the final results\n",
    "vanilla_final_results = pd.read_csv('../data/processed/vanilla_final_results.csv')\n",
    "vanilla_final_results.set_index('product', inplace=True)\n",
    "\n",
    "coded_final_results = pd.read_csv('../data/processed/coded_final_results.csv')\n",
    "coded_final_results.set_index('product', inplace=True)\n",
    "\n",
    "valence_final_results = pd.read_csv('../data/processed/valence_final_results.csv')\n",
    "valence_final_results.set_index('product', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Inputs Grid Search & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION - GRID SEARCH CAN TAKE MANY HOURS TO RUN\n",
    "# ONLY RUN ON FIRST PASS\n",
    "# create a dataframe to house the results of the model tuning from an initial grid search\n",
    "\n",
    "vanilla_gs_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                           'passes', 'per-word bounds', \n",
    "                                           'perplexity', 'topic diff', \n",
    "                                           'final perplexity', 'final topic diff', \n",
    "                                           'perplexity decreasing', 'coherence', \n",
    "                                           'top_n removed', 'n_above threshold'])\n",
    "\n",
    "# for each of the top ten products, grid search over a combination of n_passes and n_topics \n",
    "# save the parameter combinations (and saved model) of the model with the highest coherence score\n",
    "for product in top_ten[0:1]:\n",
    "    output = lda_funcs.tune_lda(df=df, product=product, n_passes=[50, 80], \n",
    "                                n_topics=[6, 8, 10, 12], save_path='vanilla_outputs',\n",
    "                                input_text='clean_vanilla', n_below=0, \n",
    "                                top_n=[2, 10], n_above=[0.5, 1.0])\n",
    "    vanilla_gs_results = lda_funcs.save_best(output, vanilla_gs_results, \n",
    "                                             save_path='vanilla_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_gs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY ON FIRST PASS\n",
    "# save off the results with the best model (highest coherence) for each product\n",
    "# examine the results\n",
    "vanilla_gs_results.to_csv('../data/interim/vanilla_gs_results.csv')\n",
    "vanilla_gs_results[['product','coherence', 'num_topics', \n",
    "                    'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "vanilla_gs_results = pd.read_csv('../data/interim/vanilla_gs_results.csv')\n",
    "vanilla_gs_results[['product','coherence', 'num_topics', \n",
    "                    'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after reviewing the results of the initial grid search pass, and manually tune the models for each of the products to try to reach a threshold of 0.5 for the final model coherence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell for the products that needs further tuning\n",
    "# increase or decrease the number of passes or topics depending on the best model found from previous results\n",
    "output = lda_funcs.tune_lda(df=df, product='B0026RQTGE', n_passes=[80], \n",
    "                            n_topics=[5, 6, 7], save_path='vanilla_outputs',\n",
    "                            input_text='clean_vanilla', n_below=0, top_n=[2], \n",
    "                            n_above=[1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the outputs\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the result is an improvement, run this cell to save it\n",
    "# save off the updated results dataframe\n",
    "vanilla_gs_results = lda_funcs.save_best(output, vanilla_gs_results, \n",
    "                                         save_path='vanilla_outputs')\n",
    "vanilla_gs_results.to_csv('../data/interim/vanilla_gs_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, save off the best results into a final output dataframe\n",
    "# create a df to collect the best models from all grid search tuning efforts\n",
    "# save it off to a csv\n",
    "vanilla_final_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                              'passes', 'per-word bounds', 'perplexity', \n",
    "                                              'topic diff', 'final perplexity', \n",
    "                                              'final topic diff', 'perplexity decreasing', \n",
    "                                              'coherence', 'top_n removed', 'n_above threshold'])\n",
    "\n",
    "for product in top_ten:\n",
    "    output = vanilla_gs_results[vanilla_gs_results['product']==product]\n",
    "    vanilla_final_results = lda_funcs.save_best(output, vanilla_final_results, \n",
    "                                                save_path='vanilla_outputs')\n",
    "\n",
    "vanilla_final_results.to_csv('../data/processed/vanilla_final_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "# load in the final results\n",
    "vanilla_final_results = pd.read_csv('../data/processed/vanilla_final_results.csv')\n",
    "vanilla_final_results.set_index('product', inplace=True)\n",
    "vanilla_final_results[['coherence', 'num_topics', 'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that success with the vanilla review (i.e., no codewords, just clean text) was pretty poor - I was not able to achieve the 0.5 goal threshold with any of the products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coded Inputs Grid Search & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION - GRID SEARCH CAN TAKE MANY HOURS TO RUN\n",
    "# ONLY RUN ON FIRST PASS\n",
    "# create a dataframe to house the results of the model tuning from an initial grid search\n",
    "\n",
    "coded_gs_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                         'passes', 'per-word bounds', 'perplexity', \n",
    "                                         'topic diff', 'final perplexity', \n",
    "                                         'final topic diff', 'perplexity decreasing',\n",
    "                                         'coherence', 'top_n removed', 'n_above threshold'])\n",
    "\n",
    "# for each of the top ten products, grid search over a combination of n_passes and n_topics \n",
    "# save the parameter combinations (and saved model) of the model with the highest coherence score\n",
    "for product in top_ten:\n",
    "    output = lda_funcs.tune_lda(df=df, product=product, n_passes=[50, 80], \n",
    "                                n_topics=[6, 8, 10, 12], save_path='coded_outputs',\n",
    "                                input_text='clean_coded', n_below=0, \n",
    "                                top_n=[2, 10], n_above=[0.5, 1.0])\n",
    "    coded_gs_results = lda_funcs.save_best(output, coded_gs_results, save_path='coded_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_gs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY ON FIRST PASS\n",
    "# save off the results with the best model (highest coherence) for each product\n",
    "# examine the results\n",
    "coded_gs_results.to_csv('../data/interim/coded_gs_results.csv')\n",
    "coded_gs_results[['product','coherence', 'num_topics', \n",
    "                  'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "coded_gs_results = pd.read_csv('../data/interim/coded_gs_results.csv')\n",
    "coded_gs_results[['product','coherence', 'num_topics', \n",
    "                  'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after reviewing the results of the initial grid search pass, and manually tune the models for each of the products to try to reach a threshold of 0.5 for the final model coherence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell for the product that needs further tuning\n",
    "output = lda_funcs.tune_lda(df=df, product='B000KV61FC', n_passes=[50], \n",
    "                            n_topics=[9, 10, 11], save_path='coded_outputs',\n",
    "                            input_text='clean_coded', n_below=0, \n",
    "                            top_n=[2], n_above=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the outputs\n",
    "output[['product','coherence', 'num_topics', \n",
    "        'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the result is an improvement, run this cell to save it\n",
    "# save off the updated results dataframe\n",
    "coded_gs_results = lda_funcs.save_best(output, coded_gs_results, \n",
    "                                       save_path='coded_outputs')\n",
    "coded_gs_results.to_csv('../data/interim/coded_gs_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, save off the best results into a final output dataframe\n",
    "# create a df to collect the best models from all grid search tuning efforts\n",
    "# save it off to a csv\n",
    "coded_final_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                            'passes', 'per-word bounds', \n",
    "                                            'perplexity', 'topic diff', \n",
    "                                            'final perplexity', 'final topic diff', \n",
    "                                            'perplexity decreasing', 'coherence', \n",
    "                                            'top_n removed', 'n_above threshold'])\n",
    "\n",
    "for product in top_ten:\n",
    "    output = coded_gs_results[coded_gs_results['product']==product]\n",
    "    coded_final_results = lda_funcs.save_best(output, coded_final_results, \n",
    "                                              save_path='coded_outputs')\n",
    "\n",
    "coded_final_results.to_csv('../data/processed/coded_final_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "# load in the final results\n",
    "coded_final_results = pd.read_csv('../data/processed/coded_final_results.csv')\n",
    "coded_final_results.set_index('product', inplace=True)\n",
    "coded_final_results[['coherence', 'num_topics', 'passes', \n",
    "                     'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the coded review (i.e., with \"GOODREVIEW\" and \"BADREVIEW\" inserted following each positive or negative word), the coherence results are somewhat better than the uncoded reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valence Coded Inputs Grid Search & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION - GRID SEARCH CAN TAKE MANY HOURS TO RUN\n",
    "# ONLY RUN ON FIRST PASS\n",
    "# create a dataframe to house the results of the model tuning from an initial grid search\n",
    "\n",
    "valence_gs_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                           'passes', 'per-word bounds', \n",
    "                                           'perplexity', 'topic diff', \n",
    "                                           'final perplexity', 'final topic diff', \n",
    "                                           'perplexity decreasing', 'coherence', \n",
    "                                           'top_n removed', 'n_above threshold'])\n",
    "\n",
    "# for each of the top ten products, grid search over a combination of n_passes and n_topics \n",
    "# save the parameter combinations (and saved model) of the model with the highest coherence score\n",
    "for product in top_ten:\n",
    "    output = lda_funcs.tune_lda(df=df, product=product, n_passes=[50, 80], \n",
    "                                n_topics=[6, 8, 10, 12], save_path='valence_outputs', \n",
    "                                input_text='clean_valence', n_below=0, \n",
    "                                top_n=[2,10], n_above=[0.5, 1.0])\n",
    "    valence_gs_results = lda_funcs.save_best(output, valence_gs_results, \n",
    "                                             save_path='valence_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_gs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY ON FIRST PASS\n",
    "# save off the results with the best model (highest coherence) for each product\n",
    "# examine the results\n",
    "valence_gs_results.to_csv('../data/interim/valence_gs_results.csv')\n",
    "valence_gs_results[['product','coherence', 'num_topics', \n",
    "                    'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "valence_gs_results = pd.read_csv('../data/interim/valence_gs_results.csv')\n",
    "valence_gs_results[['product','coherence', 'num_topics', \n",
    "                    'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after reviewing the results of the initial grid search pass, and manually tune the models for each of the products to try to reach a threshold of 0.5 for the final model coherence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell for the product that needs further tuning\n",
    "output = lda_funcs.tune_lda(df=df, product='B001EO5Q64', n_passes=[50], \n",
    "                            n_topics=[6], save_path='valence_outputs',\n",
    "                            input_text='clean_valence', n_below=0,\n",
    "                           top_n=[10], n_above=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the outputs\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the result is an improvement, run this cell to save it\n",
    "# save off the updated results dataframe\n",
    "valence_gs_results = lda_funcs.save_best(output, valence_gs_results, save_path=\"valence_outputs\")\n",
    "valence_gs_results.to_csv('../data/interim/valence_gs_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, save off the best results into a final output dataframe\n",
    "# create a df to collect the best models from all grid search tuning efforts\n",
    "# save it off to a csv\n",
    "valence_final_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                              'passes', 'per-word bounds', \n",
    "                                              'perplexity', 'topic diff',\n",
    "                                              'final perplexity', 'final topic diff', \n",
    "                                              'perplexity decreasing', 'coherence', \n",
    "                                              'top_n removed', 'n_above threshold'])\n",
    "\n",
    "for product in top_ten:\n",
    "    output = valence_gs_results[valence_gs_results['product']==product]\n",
    "    valence_final_results = lda_funcs.save_best(output, valence_final_results, \n",
    "                                                save_path='valence_outputs')\n",
    "\n",
    "valence_final_results.to_csv('../data/processed/valence_final_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_final_results.loc[0, 'top_n removed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "# load in the final results\n",
    "valence_final_results = pd.read_csv('../data/processed/valence_final_results.csv')\n",
    "valence_final_results.set_index('product', inplace=True)\n",
    "valence_final_results[['coherence', 'num_topics', 'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
