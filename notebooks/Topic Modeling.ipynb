{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import logging\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "from src.lda import lda_funcs\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "handler = logging.FileHandler('../models/training_output.log')\n",
    "\n",
    "# The handler above is somthing I needed with respect to logging.\n",
    "# Gensim performs various calculations while training the LDA model that I am using, but the only way to see them\n",
    "# is in the logging outputs.\n",
    "# Specifically, I need to capture the perplexity values during training to verify that perplexity is decreasing.\n",
    "# This metric is needed to compare models and to do hyperparameter tuning. \n",
    "\n",
    "\n",
    "# The following blog post was helpful to me in figure out how to make the log handler I needed.\n",
    "# https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>...</th>\n",
       "      <th>clean_vanilla_x</th>\n",
       "      <th>clean_coded_x</th>\n",
       "      <th>clean_valence_x</th>\n",
       "      <th>codecount_GOOD</th>\n",
       "      <th>codecount_BAD</th>\n",
       "      <th>valencecount_GOOD</th>\n",
       "      <th>valencecount_VGOOD</th>\n",
       "      <th>valencecount_BAD</th>\n",
       "      <th>valencecount_VBAD</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10108</td>\n",
       "      <td>10108</td>\n",
       "      <td>21738</td>\n",
       "      <td>B000KV61FC</td>\n",
       "      <td>A1Y1YYH71TPYC6</td>\n",
       "      <td>thefinfan54</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1222905600</td>\n",
       "      <td>...</td>\n",
       "      <td>two small dog absolutely love tug jug many us ...</td>\n",
       "      <td>two small dog absolutely love GOODREVIEW tug j...</td>\n",
       "      <td>two small dog absolutely love VGOODREVIEW tug ...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10109</td>\n",
       "      <td>10109</td>\n",
       "      <td>21739</td>\n",
       "      <td>B000KV61FC</td>\n",
       "      <td>A1SLLKDKCZ5IPL</td>\n",
       "      <td>C. Guariglia</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1221091200</td>\n",
       "      <td>...</td>\n",
       "      <td>idea basically good one however large papillon...</td>\n",
       "      <td>idea basically good GOODREVIEW one however lar...</td>\n",
       "      <td>idea basically good GOODREVIEW one however lar...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0.1.1     Id   ProductId          UserId  \\\n",
       "0         10108           10108  21738  B000KV61FC  A1Y1YYH71TPYC6   \n",
       "1         10109           10109  21739  B000KV61FC  A1SLLKDKCZ5IPL   \n",
       "\n",
       "    ProfileName  HelpfulnessNumerator  HelpfulnessDenominator  Score  \\\n",
       "0   thefinfan54                     2                       2      5   \n",
       "1  C. Guariglia                     2                       2      2   \n",
       "\n",
       "         Time    ...                                       clean_vanilla_x  \\\n",
       "0  1222905600    ...     two small dog absolutely love tug jug many us ...   \n",
       "1  1221091200    ...     idea basically good one however large papillon...   \n",
       "\n",
       "                                       clean_coded_x  \\\n",
       "0  two small dog absolutely love GOODREVIEW tug j...   \n",
       "1  idea basically good GOODREVIEW one however lar...   \n",
       "\n",
       "                                     clean_valence_x codecount_GOOD  \\\n",
       "0  two small dog absolutely love VGOODREVIEW tug ...              7   \n",
       "1  idea basically good GOODREVIEW one however lar...              3   \n",
       "\n",
       "  codecount_BAD valencecount_GOOD valencecount_VGOOD valencecount_BAD  \\\n",
       "0             3                 7                  3                3   \n",
       "1             0                 3                  0                0   \n",
       "\n",
       "  valencecount_VBAD Sentiment  \n",
       "0                 0         1  \n",
       "1                 0         0  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/reviews.csv', index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B002IEZJMA    487\n",
       "B006MONQMC    491\n",
       "B005ZBZLT4    506\n",
       "B003GTR8IO    530\n",
       "B005K4Q34S    541\n",
       "B0013A0QXC    542\n",
       "B000NMJWZO    542\n",
       "B000KV61FC    556\n",
       "B001EO5Q64    567\n",
       "B0026RQTGE    630\n",
       "Name: ProductId, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get review counts for each product\n",
    "review_counts = df['ProductId'].value_counts().sort_values()\n",
    "review_counts.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a product group to work with - the 10 products with the largest number of reviews\n",
    "top_ten = review_counts.tail(10).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Grid Search & Tuning\n",
    "\n",
    "For each of the input text types (vanilla, coded, and valence coded), first run an initial grid search with 50 or 80 training passes and either 6, 10, or 12 topics.\n",
    "\n",
    "NOTE: grid search can take hours to run fully - don't run the below cells unless you actually want to perform the searching and tuning. The csv results files can simply be loaded in where noted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "# load in the final results\n",
    "vanilla_final_results = pd.read_csv('../data/processed/vanilla_final_results.csv')\n",
    "vanilla_final_results.set_index('product', inplace=True)\n",
    "\n",
    "coded_final_results = pd.read_csv('../data/processed/coded_final_results.csv')\n",
    "coded_final_results.set_index('product', inplace=True)\n",
    "\n",
    "valence_final_results = pd.read_csv('../data/processed/valence_final_results.csv')\n",
    "valence_final_results.set_index('product', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Inputs Grid Search & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION - GRID SEARCH CAN TAKE MANY HOURS TO RUN\n",
    "# ONLY RUN ON FIRST PASS\n",
    "# create a dataframe to house the results of the model tuning from an initial grid search\n",
    "\n",
    "vanilla_gs_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                           'passes', 'per-word bounds', \n",
    "                                           'perplexity', 'topic diff', \n",
    "                                           'final perplexity', 'final topic diff', \n",
    "                                           'perplexity decreasing', 'coherence', \n",
    "                                           'top_n removed', 'n_above threshold'])\n",
    "\n",
    "# for each of the top ten products, grid search over a combination of n_passes and n_topics \n",
    "# save the parameter combinations (and saved model) of the model with the highest coherence score\n",
    "for product in top_ten[0:1]:\n",
    "    output = lda_funcs.tune_lda(df=df, product=product, n_passes=[50, 80], \n",
    "                                n_topics=[6, 8, 10, 12], save_path='vanilla_outputs',\n",
    "                                input_text='clean_vanilla', n_below=0, \n",
    "                                top_n=[2, 10], n_above=[0.5, 1.0])\n",
    "    vanilla_gs_results = lda_funcs.save_best(output, vanilla_gs_results, \n",
    "                                             save_path='vanilla_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_gs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY ON FIRST PASS\n",
    "# save off the results with the best model (highest coherence) for each product\n",
    "# examine the results\n",
    "vanilla_gs_results.to_csv('../data/interim/vanilla_gs_results.csv')\n",
    "vanilla_gs_results[['product','coherence', 'num_topics', \n",
    "                    'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "vanilla_gs_results = pd.read_csv('../data/interim/vanilla_gs_results.csv')\n",
    "vanilla_gs_results[['product','coherence', 'num_topics', \n",
    "                    'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after reviewing the results of the initial grid search pass, and manually tune the models for each of the products to try to reach a threshold of 0.5 for the final model coherence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell for the products that needs further tuning\n",
    "# increase or decrease the number of passes or topics depending on the best model found from previous results\n",
    "output = lda_funcs.tune_lda(df=df, product='B0026RQTGE', n_passes=[80], \n",
    "                            n_topics=[5, 6, 7], save_path='vanilla_outputs',\n",
    "                            input_text='clean_vanilla', n_below=0, top_n=[2], \n",
    "                            n_above=[1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the outputs\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the result is an improvement, run this cell to save it\n",
    "# save off the updated results dataframe\n",
    "vanilla_gs_results = lda_funcs.save_best(output, vanilla_gs_results, \n",
    "                                         save_path='vanilla_outputs')\n",
    "vanilla_gs_results.to_csv('../data/interim/vanilla_gs_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, save off the best results into a final output dataframe\n",
    "# create a df to collect the best models from all grid search tuning efforts\n",
    "# save it off to a csv\n",
    "vanilla_final_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                              'passes', 'per-word bounds', 'perplexity', \n",
    "                                              'topic diff', 'final perplexity', \n",
    "                                              'final topic diff', 'perplexity decreasing', \n",
    "                                              'coherence', 'top_n removed', 'n_above threshold'])\n",
    "\n",
    "for product in top_ten:\n",
    "    output = vanilla_gs_results[vanilla_gs_results['product']==product]\n",
    "    vanilla_final_results = lda_funcs.save_best(output, vanilla_final_results, \n",
    "                                                save_path='vanilla_outputs')\n",
    "\n",
    "vanilla_final_results.to_csv('../data/processed/vanilla_final_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "# load in the final results\n",
    "vanilla_final_results = pd.read_csv('../data/processed/vanilla_final_results.csv')\n",
    "vanilla_final_results.set_index('product', inplace=True)\n",
    "vanilla_final_results[['coherence', 'num_topics', 'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that success with the vanilla review (i.e., no codewords, just clean text) was pretty poor - I was not able to achieve the 0.5 goal threshold with any of the products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coded Inputs Grid Search & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION - GRID SEARCH CAN TAKE MANY HOURS TO RUN\n",
    "# ONLY RUN ON FIRST PASS\n",
    "# create a dataframe to house the results of the model tuning from an initial grid search\n",
    "\n",
    "coded_gs_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                         'passes', 'per-word bounds', 'perplexity', \n",
    "                                         'topic diff', 'final perplexity', \n",
    "                                         'final topic diff', 'perplexity decreasing',\n",
    "                                         'coherence', 'top_n removed', 'n_above threshold'])\n",
    "\n",
    "# for each of the top ten products, grid search over a combination of n_passes and n_topics \n",
    "# save the parameter combinations (and saved model) of the model with the highest coherence score\n",
    "for product in top_ten:\n",
    "    output = lda_funcs.tune_lda(df=df, product=product, n_passes=[50, 80], \n",
    "                                n_topics=[6, 8, 10, 12], save_path='coded_outputs',\n",
    "                                input_text='clean_coded', n_below=0, \n",
    "                                top_n=[2, 10], n_above=[0.5, 1.0])\n",
    "    coded_gs_results = lda_funcs.save_best(output, coded_gs_results, save_path='coded_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_gs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY ON FIRST PASS\n",
    "# save off the results with the best model (highest coherence) for each product\n",
    "# examine the results\n",
    "coded_gs_results.to_csv('../data/interim/coded_gs_results.csv')\n",
    "coded_gs_results[['product','coherence', 'num_topics', \n",
    "                  'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "coded_gs_results = pd.read_csv('../data/interim/coded_gs_results.csv')\n",
    "coded_gs_results[['product','coherence', 'num_topics', \n",
    "                  'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after reviewing the results of the initial grid search pass, and manually tune the models for each of the products to try to reach a threshold of 0.5 for the final model coherence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell for the product that needs further tuning\n",
    "output = lda_funcs.tune_lda(df=df, product='B000KV61FC', n_passes=[50], \n",
    "                            n_topics=[9, 10, 11], save_path='coded_outputs',\n",
    "                            input_text='clean_coded', n_below=0, \n",
    "                            top_n=[2], n_above=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the outputs\n",
    "output[['product','coherence', 'num_topics', \n",
    "        'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the result is an improvement, run this cell to save it\n",
    "# save off the updated results dataframe\n",
    "coded_gs_results = lda_funcs.save_best(output, coded_gs_results, \n",
    "                                       save_path='coded_outputs')\n",
    "coded_gs_results.to_csv('../data/interim/coded_gs_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, save off the best results into a final output dataframe\n",
    "# create a df to collect the best models from all grid search tuning efforts\n",
    "# save it off to a csv\n",
    "coded_final_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                            'passes', 'per-word bounds', \n",
    "                                            'perplexity', 'topic diff', \n",
    "                                            'final perplexity', 'final topic diff', \n",
    "                                            'perplexity decreasing', 'coherence', \n",
    "                                            'top_n removed', 'n_above threshold'])\n",
    "\n",
    "for product in top_ten:\n",
    "    output = coded_gs_results[coded_gs_results['product']==product]\n",
    "    coded_final_results = lda_funcs.save_best(output, coded_final_results, \n",
    "                                              save_path='coded_outputs')\n",
    "\n",
    "coded_final_results.to_csv('../data/processed/coded_final_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "# load in the final results\n",
    "coded_final_results = pd.read_csv('../data/processed/coded_final_results.csv')\n",
    "coded_final_results.set_index('product', inplace=True)\n",
    "coded_final_results[['coherence', 'num_topics', 'passes', \n",
    "                     'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the coded review (i.e., with \"GOODREVIEW\" and \"BADREVIEW\" inserted following each positive or negative word), the coherence results are somewhat better than the uncoded reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valence Coded Inputs Grid Search & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION - GRID SEARCH CAN TAKE MANY HOURS TO RUN\n",
    "# ONLY RUN ON FIRST PASS\n",
    "# create a dataframe to house the results of the model tuning from an initial grid search\n",
    "\n",
    "valence_gs_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                           'passes', 'per-word bounds', \n",
    "                                           'perplexity', 'topic diff', \n",
    "                                           'final perplexity', 'final topic diff', \n",
    "                                           'perplexity decreasing', 'coherence', \n",
    "                                           'top_n removed', 'n_above threshold'])\n",
    "\n",
    "# for each of the top ten products, grid search over a combination of n_passes and n_topics \n",
    "# save the parameter combinations (and saved model) of the model with the highest coherence score\n",
    "for product in top_ten:\n",
    "    output = lda_funcs.tune_lda(df=df, product=product, n_passes=[50, 80], \n",
    "                                n_topics=[6, 8, 10, 12], save_path='valence_outputs', \n",
    "                                input_text='clean_valence', n_below=0, \n",
    "                                top_n=[2,10], n_above=[0.5, 1.0])\n",
    "    valence_gs_results = lda_funcs.save_best(output, valence_gs_results, \n",
    "                                             save_path='valence_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_gs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY ON FIRST PASS\n",
    "# save off the results with the best model (highest coherence) for each product\n",
    "# examine the results\n",
    "valence_gs_results.to_csv('../data/interim/valence_gs_results.csv')\n",
    "valence_gs_results[['product','coherence', 'num_topics', \n",
    "                    'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "valence_gs_results = pd.read_csv('../data/interim/valence_gs_results.csv')\n",
    "valence_gs_results[['product','coherence', 'num_topics', \n",
    "                    'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after reviewing the results of the initial grid search pass, and manually tune the models for each of the products to try to reach a threshold of 0.5 for the final model coherence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell for the product that needs further tuning\n",
    "output = lda_funcs.tune_lda(df=df, product='B001EO5Q64', n_passes=[50], \n",
    "                            n_topics=[6], save_path='valence_outputs',\n",
    "                            input_text='clean_valence', n_below=0,\n",
    "                           top_n=[10], n_above=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the outputs\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the result is an improvement, run this cell to save it\n",
    "# save off the updated results dataframe\n",
    "valence_gs_results = lda_funcs.save_best(output, valence_gs_results, save_path=\"valence_outputs\")\n",
    "valence_gs_results.to_csv('../data/interim/valence_gs_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, save off the best results into a final output dataframe\n",
    "# create a df to collect the best models from all grid search tuning efforts\n",
    "# save it off to a csv\n",
    "valence_final_results = pd.DataFrame(columns=['product', 'num_topics', 'chunk', \n",
    "                                              'passes', 'per-word bounds', \n",
    "                                              'perplexity', 'topic diff',\n",
    "                                              'final perplexity', 'final topic diff', \n",
    "                                              'perplexity decreasing', 'coherence', \n",
    "                                              'top_n removed', 'n_above threshold'])\n",
    "\n",
    "for product in top_ten:\n",
    "    output = valence_gs_results[valence_gs_results['product']==product]\n",
    "    valence_final_results = lda_funcs.save_best(output, valence_final_results, \n",
    "                                                save_path='valence_outputs')\n",
    "\n",
    "valence_final_results.to_csv('../data/processed/valence_final_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_final_results.loc[0, 'top_n removed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS\n",
    "# load in the final results\n",
    "valence_final_results = pd.read_csv('../data/processed/valence_final_results.csv')\n",
    "valence_final_results.set_index('product', inplace=True)\n",
    "valence_final_results[['coherence', 'num_topics', 'passes', 'top_n removed', 'n_above threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
