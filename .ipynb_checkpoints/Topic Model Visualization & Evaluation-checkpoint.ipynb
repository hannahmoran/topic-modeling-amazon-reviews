{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"f87677b5-55b2-4a34-a937-705a655d8cd8\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"f87677b5-55b2-4a34-a937-705a655d8cd8\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"f87677b5-55b2-4a34-a937-705a655d8cd8\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'f87677b5-55b2-4a34-a937-705a655d8cd8' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"f87677b5-55b2-4a34-a937-705a655d8cd8\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"f87677b5-55b2-4a34-a937-705a655d8cd8\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"f87677b5-55b2-4a34-a937-705a655d8cd8\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'f87677b5-55b2-4a34-a937-705a655d8cd8' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"f87677b5-55b2-4a34-a937-705a655d8cd8\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "% matplotlib inline\n",
    "\n",
    "import topic_dist\n",
    "import topic_funcs\n",
    "import wordcloud_vis as wcv\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, TrigramAssocMeasures, QuadgramCollocationFinder, BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk import word_tokenize, FreqDist, bigrams\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file\n",
    "from bokeh.models import ColumnDataSource, LabelSet, Jitter, HoverTool, Range1d, LinearAxis\n",
    "from bokeh.palettes import GnBu3, OrRd3\n",
    "from bokeh.core.properties import value\n",
    "from bokeh.transform import dodge, jitter\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import logging\n",
    "from six import itervalues\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'reviews_subset.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ae1a7938da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load in data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reviews_subset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreview_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ProductId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtop_ten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'reviews_subset.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "df = pd.read_csv('reviews_subset.csv', index_col=0)\n",
    "review_counts = df['ProductId'].value_counts().sort_values()\n",
    "top_ten = review_counts.tail(10).index.values\n",
    "\n",
    "vanilla_final_results = pd.read_csv('vanilla_final_results.csv')\n",
    "vanilla_final_results.set_index('product', inplace=True)\n",
    "\n",
    "coded_final_results = pd.read_csv('coded_final_results.csv')\n",
    "coded_final_results.set_index('product', inplace=True)\n",
    "\n",
    "valence_final_results = pd.read_csv('valence_final_results.csv')\n",
    "valence_final_results.set_index('product', inplace=True)\n",
    "\n",
    "vanilla_topic_data = pd.read_csv('vanilla_topic_data.csv')\n",
    "coded_topic_data = pd.read_csv('coded_topic_data.csv')\n",
    "valence_topic_data = pd.read_csv('valence_topic_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Topic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_coherence = vanilla_final_results['coherence']\n",
    "coded_coherence = coded_final_results['coherence']\n",
    "valence_coherence = valence_final_results['coherence']\n",
    "vanilla_ts = vanilla_final_results['num_topics']\n",
    "coded_ts = coded_final_results['num_topics']\n",
    "valence_ts = valence_final_results['num_topics']\n",
    "\n",
    "data_coherence = {'products': top_ten,\n",
    "                  'vanilla': vanilla_coherence,\n",
    "                  'coded': coded_coherence,\n",
    "                  'valence': valence_coherence}\n",
    "\n",
    "source_coherence = ColumnDataSource(data=data_coherence)\n",
    "\n",
    "data_topics = {'products': top_ten,\n",
    "               'vanilla': vanilla_ts,\n",
    "               'coded': coded_ts,\n",
    "               'valence': valence_ts}\n",
    "\n",
    "source_topics = ColumnDataSource(data=data_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(x_range=top_ten, y_range=(0, 0.7), \n",
    "           plot_height=350, \n",
    "           title='Coherence by Product',\n",
    "           toolbar_location=None, tools='')\n",
    "\n",
    "p.vbar(x=dodge('products', -0.25, range=p.x_range), \n",
    "       top='vanilla', width=0.2, source=source_coherence,\n",
    "       color='#c9d9d3', legend=value('Vanilla Reviews'))\n",
    "\n",
    "p.vbar(x=dodge('products',  0.0,  range=p.x_range), \n",
    "       top='coded', width=0.2, source=source_coherence,\n",
    "       color='#718dbf', legend=value('Coded Reviews'))\n",
    "\n",
    "p.vbar(x=dodge('products',  0.25, range=p.x_range), \n",
    "       top='valence', width=0.2, source=source_coherence,\n",
    "       color='#e84d60', legend=value('Valence Coded Reviews'))\n",
    "\n",
    "p.x_range.range_padding = 0.1\n",
    "p.xgrid.grid_line_color = None\n",
    "p.legend.location = 'top_left'\n",
    "p.legend.orientation = 'horizontal'\n",
    "p.xaxis.major_label_orientation = 0.75\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In only one case is there a product where the best average topic coherence was achieved with the vanilla review inputs. For six of the ten products the average topic coherence from the coded review set is higher than for the valence coded reviews, and in the three cases where the valence input set has the highest coherence, it's by a very slim margin over the coded input set. So at first blush, it appears that the simple encoding has produced the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(x_range=top_ten, y_range=(0, 20), \n",
    "           plot_height=350, \n",
    "           title='Number of Topics by Product',\n",
    "           toolbar_location=None, tools='')\n",
    "\n",
    "p.vbar(x=dodge('products', -0.25, range=p.x_range), \n",
    "       top='vanilla', width=0.2, source=source_topics,\n",
    "       color='#c9d9d3', legend=value('Vanilla Reviews'))\n",
    "\n",
    "p.vbar(x=dodge('products',  0.0,  range=p.x_range), \n",
    "       top='coded', width=0.2, source=source_topics,\n",
    "       color='#718dbf', legend=value('Coded Reviews'))\n",
    "\n",
    "p.vbar(x=dodge('products',  0.25, range=p.x_range), \n",
    "       top='valence', width=0.2, source=source_topics,\n",
    "       color='#e84d60', legend=value('Valence Coded Reviews'))\n",
    "\n",
    "p.x_range.range_padding = 0.1\n",
    "p.xgrid.grid_line_color = None\n",
    "p.legend.location = 'top_left'\n",
    "p.legend.orientation = 'horizontal'\n",
    "p.xaxis.major_label_orientation = 0.75\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no clear patterns with respect to the number of topics that optimized the average topic coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Quality Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Assignment\n",
    "First, how are the reviews for each product distributed among the topics discovered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in top_ten:\n",
    "    topic_dist.assign_topics(df, product, vanilla_final_results, \n",
    "                             'vanilla_outputs', 'clean_vanilla', 'Vanilla')\n",
    "    topic_dist.assign_topics(df, product, coded_final_results, \n",
    "                             'coded_outputs', 'clean_coded', 'Coded')\n",
    "    topic_dist.assign_topics(df, product, valence_final_results, \n",
    "                             'valence_outputs', 'clean_valence', 'Valence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('reviews_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in top_ten:\n",
    "    grid = topic_dist.plot_topic_distribution(df, product)\n",
    "    show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many of the products, regardless of the input text type used, the majority of reviews fall into one or two topics. This important to note because it is the sort of class imbalance problem that would present challenges for any classification exercise (as would the fact that the reviews in the dataset are overwhelmingly positive). \n",
    "\n",
    "It also appears that in many cases, the coded input text reviews have the most severe review concentration - that is, there is greater distribution amongst topics with the vanilla and the valence coded datasets. This could mean that the encoding has worked the opposite way I want it to, and that maybe the apparent higher coherence performanc of the coded review set is not a positive thing. In using topic modeling, I am hoping to tease out various topic themes within the review set. Perhaps adding the simple codewords has simply resulted in topics that are mostly composed of negative or positive words. This means I need to further investigate the quality of the topics produced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Mixture Visualization\n",
    "\n",
    "In doing the topic assignment above, I took an initial simple approach of just assigning the review to the topic with the highest probability of having generated that topic. How high are those values? Do most reviews have a very high likelihood of coming from a single topic, or are there reviews that are equally likely to have been generated by 2 or 3 topics? \n",
    "\n",
    "This is especially important because of the concentration seen above in the topic assignments. I don't necessarily want to consider a review that has a 0.9 max fit value for a topic in the same way as a review with a max 0.5 fit value. If the top 2 or 3 fit values for many of the reviews are very close, it means that there may not be as much concentration in topics as appears from charts using a simple assignment method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def topic_mix_histogram(df, product, encoding_type):\n",
    "    hist, edges = np.histogram(df.loc[df['ProductId']==product, \n",
    "                               '{} Fit'.format(encoding_type)], density=False, bins=10)\n",
    "    \n",
    "    p = figure(plot_height=150, plot_width=150, x_range=[0,1], y_range=[0,200])\n",
    "    p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color='white')\n",
    "    return p\n",
    "gridlist = []\n",
    "for product in top_ten:\n",
    "    vanilla = topic_mix_histogram(df, product, 'Vanilla')\n",
    "    vanilla.yaxis.axis_label = product\n",
    "    coded = topic_mix_histogram(df, product, 'Coded')\n",
    "    valence = topic_mix_histogram(df, product, 'Valence')\n",
    "    if product == top_ten[0]:\n",
    "        vanilla.title.text='Vanilla Fit Values'\n",
    "        vanilla.title.text_font_size='9pt'\n",
    "        coded.title.text = 'Coded Fit Values'\n",
    "        coded.title.text_font_size='9pt'\n",
    "        valence.title.text = 'Valence Fit Values'\n",
    "        valence.title.text_font_size='9pt'\n",
    "\n",
    "    #grid = gridplot([[vanilla, coded, valence]])\n",
    "    #print(\"Topic Mixture Fit Values for Product {}\".format(product))\n",
    "    #show(grid)\n",
    "    gridlist.append([vanilla, coded, valence])\n",
    "\n",
    "    \n",
    "grid=gridplot(gridlist)\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import export_png\n",
    "export_png(grid, filename='topic_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above show the highest topic fit value for each review, by product and by input text type. If the topics produced in the topic modeling were coherent and highly distinct from one another, these histograms would be skewed, with more of the values falling closer to a value of 1. Values closer to 1 indicate higher probability that the topic generated the review. \n",
    "\n",
    "Indeed, there are certainly plenty of product / input text combinations where the values are concentrated around values less than 0.7. Just from eyeballing the charts it seems that the coded inputs are typically showing more concentration around 0.9, and the vanilla and valence input histograms look pretty similar for many of the products. This fits with observations above that the topic models using the coded inputs are tending to have higher coherence - it will require a more qualitative analysis approach to say why this is true and whether it is a good thing. \n",
    "\n",
    "These patterns are visualized another way in the charts below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_plot_sub(df, topic_data, product, encoding_type):\n",
    "    data = df[df['ProductId']==product]\n",
    "    topics = [t for t in topic_data.loc[topic_data['product']==product, 'topic'].values]\n",
    "\n",
    "    x_topics = ['Topic {}'.format(t) for t in topics]\n",
    "\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    for t in topics:\n",
    "        t_data = data.loc[data['{} Topic'.format(encoding_type)]==t]\n",
    "        # If the max fit value is greater than or equal to 0.7, call it the main topic\n",
    "        main_topic_count = len(t_data.loc[t_data['{} Fit'.format(encoding_type)]>=0.7])\n",
    "        # If the max fit value is less than 0.7, call it a subtopic\n",
    "        sub_topic_count = len(t_data.loc[t_data['{} Fit'.format(encoding_type)]<0.7])\n",
    "        y1.append(main_topic_count)\n",
    "        y2.append(sub_topic_count)\n",
    "    \n",
    "    source = ColumnDataSource(data={'topics': x_topics,\n",
    "                                    'main_topics': y1,\n",
    "                                    'sub_topics': y2})\n",
    "    \n",
    "    p = figure(x_range=x_topics, y_range=(0,500), plot_height=150, \n",
    "               plot_width=320, toolbar_location=None,\n",
    "               title='{} Reviews: {}'.format(encoding_type, product))\n",
    "    \n",
    "\n",
    "    p.yaxis.axis_label = 'Review Count'\n",
    "    \n",
    "    \n",
    "    p.vbar(x=dodge('topics', 0.0, range=p.x_range), top='main_topics', \n",
    "           source=source, width=0.3, color='red')\n",
    "    p.vbar(x=dodge('topics', 0.25, range=p.x_range), top='sub_topics', \n",
    "           source=source, width = 0.3, color='blue')\n",
    "\n",
    "    p.xaxis.major_label_orientation = 0.75\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_ten' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-41debcb4f569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mgridlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_ten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_plot_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvanilla_topic_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Vanilla'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_ten' is not defined"
     ]
    }
   ],
   "source": [
    "gridlist = []\n",
    "\n",
    "def label_plot(title):    \n",
    "    p = figure(y_range=[0.1,1], plot_height=100, plot_width=320, toolbar_location=None, title=title)\n",
    "    p.vbar(0, color='red', width=0.4, top=0, legend=\"Fit > 0.7\") \n",
    "    p.vbar(0, color='blue', width=0.4, top=0, legend=\"Fit < 0.7\")\n",
    "    p.legend.location ='center'\n",
    "    p.legend.label_text_font_size='6pt'\n",
    "    p.yaxis.visible=False\n",
    "    p.xaxis.visible=False\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.outline_line_color=None\n",
    "    p.legend.orientation = \"horizontal\"\n",
    "    return p\n",
    "\n",
    "l = label_plot(\"Vanilla Reviews\")\n",
    "c = label_plot(\"Coded Reviews\")\n",
    "r = label_plot(\"Valence Reviews\")\n",
    "\n",
    "\n",
    "\n",
    "gridlist.append([l,c,r])\n",
    "for product in top_ten:\n",
    "    \n",
    "    l = construct_plot_sub(df, vanilla_topic_data, product, 'Vanilla')\n",
    "    c = construct_plot_sub(df, coded_topic_data, product, 'Coded')\n",
    "    r = construct_plot_sub(df, valence_topic_data, product, 'Valence')\n",
    "\n",
    "    gridlist.append([l,c,r])\n",
    "    \n",
    "    #grid = gridplot([[l, c, r]])\n",
    "    #show(grid)\n",
    "grid=gridplot(gridlist)\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(grid, filename='review_dist_fitsplit.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show the number of reviews classified into each topic for each product - again, meaning the number of reviews where that topic had the maximum probability of having generated the review. The review counts are here split by the fit value itself - whether it is greater than or equal to 0.7, or below 0.7. Overall, more reviews fall above that 0.7 line than don't, across products, topics, and review encoding types, but there are certainly places where that is not true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mixture_stats = pd.DataFrame(columns = ['Vanilla Mean', 'Vanilla Median', 'Vanilla SD',\n",
    "                                             'Coded Mean', 'Coded Median', 'Coded SD',\n",
    "                                             'Valence Mean', 'Valence Median', 'Valence SD'])\n",
    "for product in top_ten:\n",
    "    vanilla_data = df.loc[df['ProductId']==product, 'Vanilla Fit']\n",
    "    coded_data = df.loc[df['ProductId']==product, 'Coded Fit']\n",
    "    valence_data = df.loc[df['ProductId']==product, 'Valence Fit']\n",
    "    data = pd.DataFrame({'Vanilla Mean': np.mean(vanilla_data),\n",
    "                         'Vanilla Median': np.median(vanilla_data),\n",
    "                         'Vanilla SD': np.std(vanilla_data), \n",
    "                         'Coded Mean': np.mean(coded_data),\n",
    "                         'Coded Median': np.median(coded_data),\n",
    "                         'Coded SD': np.std(coded_data),\n",
    "                         'Valence Mean': np.mean(valence_data),\n",
    "                         'Valence Median': np.median(valence_data),\n",
    "                         'Valence SD': np.std(valence_data)}, index=[product])\n",
    "    topic_mixture_stats = pd.concat([data, topic_mixture_stats])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mixture_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(topic_mixture_stats)\n",
    "p1 = figure(x_range =[0,1], y_range=[0, 0.25], plot_height=250, plot_width=250,\n",
    "           title=\"Topic Fit Mean vs Std Deviation\")\n",
    "p1.circle(x='Vanilla Mean', y='Vanilla SD', source=source, color='green', \n",
    "          size=8, line_color='white', legend='Vanilla')\n",
    "p1.circle(x='Coded Mean', y='Coded SD', source=source, color='blue', \n",
    "          size=8, line_color='white', legend='Coded')\n",
    "p1.circle(x='Valence Mean', y='Valence SD', source=source, color='purple', \n",
    "          size=8, line_color='white', legend='Valence')\n",
    "p1.legend.location ='bottom_left'\n",
    "p1.legend.label_text_font_size='7pt'\n",
    "\n",
    "\n",
    "p2 = figure(x_range =[0,1], y_range=[0, 0.25], plot_height=250, plot_width=250,\n",
    "           title=\"Topic Fit Median vs Std Deviation\")\n",
    "p2.circle(x='Vanilla Median', y='Vanilla SD', source=source, color='green', \n",
    "          size=8, line_color='white', legend='Vanilla')\n",
    "p2.circle(x='Coded Median', y='Coded SD', source=source, color='blue', \n",
    "          size=8, line_color='white', legend='Coded')\n",
    "p2.circle(x='Valence Median', y='Valence SD', source=source, color='purple', \n",
    "          size=8, line_color='white', legend='Valence')\n",
    "p2.legend.location ='bottom_left'\n",
    "p2.legend.label_text_font_size='7pt'\n",
    "\n",
    "\n",
    "\n",
    "grid=gridplot([[p1,p2]])\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple table and a couple of scatter plots confirm that the topic models generated from the coded input text are generating max fit values with higher mean and median values and lower standard deviation. This agrees with the idea that the topic models from this coded text are more coherent - so far, it continues to look like the coded text is outperforming the vanilla and valence coded text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Quality\n",
    "\n",
    "To continue to assess the quality of the topic models, I will collect some information about each of them on a per-product, per-topic basis, including:\n",
    "\n",
    "* the number of reviews from that dataset that were classified into that topic\n",
    "* the topic coherence\n",
    "* the 'top' (most frequently appearing) words in the topic\n",
    "* the most representative review (review in the dataset with the highest probability of having been generated by this topic model)\n",
    "* the probability value for that review\n",
    "* top bigrams and trigrams from the reviews classified to this topic, both by raw frequency as well as a measure of information (using PMI here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_topic_data = pd.DataFrame(columns = ['product', 'topic', 'review_count', \n",
    "                                             'topic_coherence', 'top_words',\n",
    "                                             'best_review', 'best_review_fit', \n",
    "                                             'top_bigrams_pmi','top_bigrams_freq',\n",
    "                                             'top_trigrams_pmi', 'top_trigrams_freq'])\n",
    "\n",
    "for product in top_ten:\n",
    "    stage_data = topic_funcs.get_topic_data(product, df, vanilla_final_results, \n",
    "                                            'clean_vanilla', 'vanilla_outputs', 'Vanilla')\n",
    "    vanilla_topic_data = pd.concat([vanilla_topic_data, stage_data])\n",
    "    del stage_data\n",
    "\n",
    "vanilla_topic_data.to_csv('vanilla_topic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_topic_data = pd.DataFrame(columns = ['product', 'topic', 'review_count', \n",
    "                                           'topic_coherence', 'top_words',\n",
    "                                           'best_review', 'best_review_fit', \n",
    "                                           'top_bigrams_pmi','top_bigrams_freq',\n",
    "                                           'top_trigrams_pmi', 'top_trigrams_freq'])\n",
    "for product in top_ten:\n",
    "    stage_data = topic_funcs.get_topic_data(product, df, coded_final_results, \n",
    "                                            'clean_coded', 'coded_outputs', 'Coded')\n",
    "    coded_topic_data = pd.concat([coded_topic_data, stage_data])\n",
    "    del stage_data\n",
    "\n",
    "coded_topic_data.to_csv('coded_topic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_topic_data = pd.DataFrame(columns = ['product', 'topic', 'review_count', \n",
    "                                             'topic_coherence', 'top_words',\n",
    "                                             'best_review', 'best_review_fit', \n",
    "                                             'top_bigrams_pmi','top_bigrams_freq',\n",
    "                                             'top_trigrams_pmi', 'top_trigrams_freq'])\n",
    "for product in top_ten:\n",
    "    stage_data = topic_funcs.get_topic_data(product, df, valence_final_results, \n",
    "                                            'clean_valence', 'valence_outputs', 'Valence')\n",
    "    valence_topic_data = pd.concat([valence_topic_data, stage_data])\n",
    "    del stage_data\n",
    "\n",
    "valence_topic_data.to_csv('valence_topic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_topic_data = pd.read_csv('vanilla_topic_data.csv')\n",
    "coded_topic_data = pd.read_csv('coded_topic_data.csv')\n",
    "valence_topic_data = pd.read_csv('valence_topic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_plot(topic_data, product, encoding_type):\n",
    "    data = topic_data[topic_data['product']==product]\n",
    "    data.sort_values(ascending=False, axis=0, by='review_count', inplace=True) \n",
    "    x = ['Topic {}'.format(t) for t in data['topic'].values]\n",
    "    y = data['review_count']\n",
    "    y2 = data['topic_coherence']\n",
    "    y3 = np.mean(y2)\n",
    "    p = figure(x_range=x, y_range=(0,550), plot_height=325, \n",
    "               plot_width=325, toolbar_location=None,\n",
    "               title='{} Reviews: Product {}'.format(encoding_type, product))\n",
    "    \n",
    "\n",
    "    p.vbar(x=x, top=y, width=0.9)\n",
    "    p.yaxis.axis_label = 'Review Count'\n",
    "    p.extra_y_ranges = {'Topic Coherence': Range1d(start=0, end=1)}\n",
    "    p.line(x=x, y=y2, color='red', y_range_name='Topic Coherence')\n",
    "    p.line(x=x, y=y3, color='black', y_range_name='Topic Coherence')\n",
    "    p.add_layout(LinearAxis(y_range_name='Topic Coherence', axis_label='Topic Coherence'), 'right')\n",
    "    p.xaxis.major_label_orientation = 0.75\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in top_ten:\n",
    "    \n",
    "    l = construct_plot(vanilla_topic_data, product, 'Vanilla')\n",
    "    c = construct_plot(coded_topic_data, product, 'Coded')\n",
    "    r = construct_plot(valence_topic_data, product, 'Valence')\n",
    "    grid = gridplot([[l, c, r]])\n",
    "    show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations about these plots:\n",
    "* topic coherence is not necessarily highest for the topic with most reviews assigned\n",
    "* as previously observed, the average topic coherence (shown by the black line) for the coded reviews tends to be a little higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Mixtures Visualization\n",
    "\n",
    "Clearly not all reviews have an overwhelming likelihood of having been generated by one topic - so there could be many reviews that have equal likelihood of having been generated by two or more different topics. To judge this, I'm adding a subtopic concept. Now I will classify reviews in three potential ways:\n",
    "* Single main topic - if the max fit value is greater than or equal to 0.7, this is the single main topic generating the review\n",
    "* Primary subtopic - if the max fit value is less than 0.7 but greater than or equal to 0.3, this is the primary subtopic generating the review\n",
    "* Secondary subtopic - if the next highest fit value for a review where the max fit value is less than or equal to 0.7, and that second highest fit value is greater than or equal to 0.3, this is the secondary subtopic generating the review\n",
    "\n",
    "\n",
    "What we are looking for in these plots is to see if there are topics that are well represented as subtopics that are not showing up as a single main topic, or even as a primary subtopic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_plot_subtopics(df, topic_data, product, encoding_type):\n",
    "    data = df[df['ProductId']==product]\n",
    "    topic_data = topic_data.loc[topic_data['product']==product]\n",
    "    topics = [t for t in topic_data['topic'].values]\n",
    "\n",
    "    x_topics = ['Topic {}'.format(t) for t in topics]\n",
    "\n",
    "    single_topic = []\n",
    "    subtopic_primary = []\n",
    "    subtopic_secondary = []\n",
    "    \n",
    "    coherence = []\n",
    "    \n",
    "    for t in topics:\n",
    "        t_data_main = data.loc[data['{} Topic'.format(encoding_type)]==t]\n",
    "        t_data_sub = data.loc[data['{} Subtopic'.format(encoding_type)]==t]\n",
    "        # If the max fit value is greater than or equal to 0.7, call it the single (main) topic\n",
    "        single = len(t_data_main.loc[t_data_main['{} Fit'.format(encoding_type)]>=0.7])\n",
    "        # If the max fit value is less than 0.7, call it a primary subtopic\n",
    "        primary = len(t_data_main.loc[(t_data_main['{} Fit'.format(encoding_type)]<0.7) &\n",
    "                                     t_data_main['{} Fit'.format(encoding_type)]>=0.3])\n",
    "        # If the subtopic's fit value is greater than or equal to 0.3, call it a secondary subtopic\n",
    "        secondary = len(t_data_sub.loc[t_data_sub['{} Subtopic Fit'.format(encoding_type)]>=0.3])\n",
    "        coh = topic_data.loc[topic_data['topic']==t, 'topic_coherence']\n",
    "        single_topic.append(single)\n",
    "        subtopic_primary.append(primary)\n",
    "        subtopic_secondary.append(secondary)\n",
    "        coherence.append(coh)\n",
    "    \n",
    "    source = ColumnDataSource(data={'topics': x_topics,\n",
    "                                    'single_topic': single_topic,\n",
    "                                    'subtopic_primary': subtopic_primary,\n",
    "                                   'subtopic_secondary': subtopic_secondary})\n",
    "    \n",
    "    p = figure(x_range=x_topics, y_range=(0,500), plot_height=250, \n",
    "               plot_width=320, toolbar_location=None,\n",
    "               title='{} Reviews: {}'.format(encoding_type, product))\n",
    "    \n",
    "\n",
    "    p.yaxis.axis_label = 'Review Count'\n",
    "    p.extra_y_ranges = {'Topic Coherence': Range1d(start=0, end=1)}\n",
    "\n",
    "    \n",
    "    p.vbar(x=dodge('topics', -0.25, range=p.x_range), top='single_topic', \n",
    "           source=source, width=0.3, color='red', legend=\"Single Primary Topic\")\n",
    "    p.vbar(x=dodge('topics', 0.0, range=p.x_range), top ='subtopic_primary',\n",
    "          source=source, width=0.3, color='green', legend=\"Primary Subtopic\")\n",
    "    p.vbar(x=dodge('topics', 0.25, range=p.x_range), top='subtopic_secondary', \n",
    "           source=source, width = 0.3, color='blue', legend=\"Secondary Subtopic\")\n",
    "\n",
    "    p.line(x=x_topics, y=coherence, color='red', y_range_name='Topic Coherence')\n",
    "    p.add_layout(LinearAxis(y_range_name='Topic Coherence', axis_label='Topic Coherence'), 'right')\n",
    "\n",
    "    \n",
    "    \n",
    "    p.xaxis.major_label_orientation = 0.75\n",
    "    p.legend.location ='top_left'\n",
    "    p.legend.label_text_font_size='6pt'\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in top_ten:\n",
    "    \n",
    "    l = construct_plot_subtopics(df, vanilla_topic_data, product, 'Vanilla')\n",
    "    c = construct_plot_subtopics(df, coded_topic_data, product, 'Coded')\n",
    "    r = construct_plot_subtopics(df, valence_topic_data, product, 'Valence')\n",
    "    grid = gridplot([[l, c, r]])\n",
    "    show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There really aren't many cases where a topic is not represented as a single main topic but *is* strongly represented as a secondary subtopic, but there are a good number of cases where a topic shows up mainly as a primary subtopic.\n",
    "\n",
    "Another key takeaway from these plots comes from overlaying the topic coherence. It turns out that the topics with the highest number of assigned reviews (using any method) are not typically the reviews with the highest coherence of the group. This is another item for investigation in the qualitative assessment of the topics' quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_mixture_plot(df, topic_data, product, encoding_type):\n",
    "    topics = [\"Topic {}\".format(t) for t in topic_data[topic_data['product']==product]['topic'].values]\n",
    "    data=df[df[\"ProductId\"]==product]\n",
    "    p = figure(x_range=topics, y_range=(0,1), plot_height=325, \n",
    "               plot_width=325, toolbar_location=None,\n",
    "               title='{} Topic Mixture: Product {}'.format(encoding_type, product))\n",
    "    for i in data.index.values:\n",
    "        r = data.loc[i]['Topic Mixtures {}'.format(encoding_type)]\n",
    "        p.line(x=list(\"Topic {}\".format(k) for k in ast.literal_eval(r).keys()), y=list(ast.literal_eval(r).values()))\n",
    "    p.xaxis.major_label_orientation = 0.75\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this in segments or refer to the charts in the report \n",
    "# as it can often cause the browser to crash\n",
    "for product in top_ten[0:5]:\n",
    "    \n",
    "    l = topic_mixture_plot(df, vanilla_topic_data, product, 'Vanilla')\n",
    "    c = topic_mixture_plot(df, coded_topic_data, product, 'Coded')\n",
    "    r = topic_mixture_plot(df, valence_topic_data, product, 'Valence')\n",
    "    grid = gridplot([[l, c, r]])\n",
    "    show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this in segments or refer to the charts in the report \n",
    "# as it can often cause the browser to crash\n",
    "for product in top_ten[5:]:\n",
    "    \n",
    "    l = topic_mixture_plot(df, vanilla_topic_data, product, 'Vanilla')\n",
    "    c = topic_mixture_plot(df, coded_topic_data, product, 'Coded')\n",
    "    r = topic_mixture_plot(df, valence_topic_data, product, 'Valence')\n",
    "    grid = gridplot([[l, c, r]])\n",
    "    show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Topic Evaluation\n",
    "\n",
    "One of the more challenging aspects of NLP work is that there is only so much that quantitative metrics can say about the quality of the outputs. No matter how a metric like coherence is defined, there is still a subjective element in trying to compare two topics or determine how meaningful a topic is, or how much sense it makes to a person. \n",
    "\n",
    "So far it is clear that\n",
    "* The coded reviews seem to produce topic sets with the highest average coherence\n",
    "* More often than not, most of the reviews for a product are not likely to have been generated by the topic with the highest coherence amongst the topic set\n",
    "* The max fit value for many reviews is actually quite low\n",
    "\n",
    "So how good are these topics? Is there any useful information that can be gleaned from them, any information that would be useful to a seller or a consumer in a practical way?\n",
    "\n",
    "To see if they're useful, we need to find a way to answer the question \"what is this topic about?\" Answering this question is actually not that easy. Do we consider the vocabulary frequency distribution of the topic model itself? Do we look at a vocabular frequency distribution using the words that actually appeard in reviews as an input? How should we take into account the fact that many topics generated in the LDA process do not appear to have actually generated any reviews in the review set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds\n",
    "\n",
    "Wordclouds are a nice, simple, visual way to get an idea of what is being talked about in a document or a set of documents. There is a library aptly named WordCloud that can be easily used to accomplish this, which simply displays the words appearing in a document or document set, where the relative size of the words corresponds to the frequency with which they appear in the document or document set. It's built to work with matplotlib so I'll be using that instead of bokeh for this section. \n",
    "\n",
    "First, let's look at why just looking at the topics themselves may not be that insightful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for product in top_ten[0:1]:\n",
    "    wcv.make_model_wordclouds(product=product, df=df, vanilla_final_results=vanilla_final_results,\n",
    "                              coded_final_results = coded_final_results,\n",
    "                              valence_final_results= valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the wordclouds, based on the frequency distribution that defines the topic, for all the topics generated by the three input text sets for the first product. We can pretty easily tell that this is some kind of coffee product, just from looking at these. \n",
    "\n",
    "These wordclouds are actually pretty readable, and seem to represent coherent topics. But, remember that these are visualizations of a frequency distribution that serves as a model for generating a bit of text. So when a word appears large, it doesn't mean that this word necessarily occurred frequently in the corpus - it means that, if we were to generate a review using this model (again a distribution of relative frequencies for words), that word would have a high probability of appearing in the generated review. Reading through the clouds, you have wonder how many reviews are actually written that would correspond to some of topics. For example, there are two different topics for the vanilla inputs where the word \"husband\" is a bizarrely prominent feature. Are there really that many people writing about their husbands in their coffee review on amazon, or is it just that there were a few reviews that included that word, along with some other words that are relatively rare in the corpus, resulting in these topics emerging?\n",
    "\n",
    "Other things emerge that are related to common difficulties in the data I've got. For example, for many of the food-related products in the review, the review will contain a copy-paste of the entire ingredients list or nutrition information. These tend to prompt a very unique topic to emerge because they appear consistently together, and only together, in the corpus. You can see this with vanilla topic 8 and valence topic 9, though interestingly there isn't really a valence topic like this. \n",
    "\n",
    "Let's compare the model wordclouds with the wordclouds from actual reviews that were \"generated\" by the topics represented above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in top_ten[0:1]:\n",
    "    wcv.make_all_wordclouds(product, df, vanilla_final_results, coded_final_results, valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I honestly hoped there were tons of reviews being generated by vanilla topic 2 where the reviewer is talking about how the coffee was so weird and nasty that they and their husband couldn't even finish it, wow! But, sadly, most of the reviews from all three input sets appear to have been generated by the very boring topic where the reviewer is basically saying, this is illy espresso and I bought it and I had some thoughts on the flavor. Which is fair because most of the time there probably isn't much else to be said. \n",
    "\n",
    "It's a similar story with the coded and valence inputs, though it appears the valence coding might have been successful in splitting out a slightly more negative set of these basic reviews.\n",
    "\n",
    "What starts to become clear as you do a few more comparisons is that the topic model wordclouds are more readable and \"topic-like,\" because when we look at wordclouds from the actual reviews we may be getting overwhelmed with things that are closer to stopwords because they're so common, but aren't very meaningful in terms of the topic. However, the review count is critical - so I'll combine the two. We'll still visualize the topic models, but only for the topics that are actually likely to have generated reviews in our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for product in top_ten[0:1]:\n",
    "    wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                            vanilla_final_results, \n",
    "                                            coded_final_results, \n",
    "                                            valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now in the chart above, we can see the wordcloud of the topic model, which indicates which words are most frequently occurring in the topic, along with the number of reviews that were actually classified to that topic. I've inserted that the review have at least a 70% probability of having been generated by the topic in order to get classified to it, not just that it have the max probability of having come from that topic. \n",
    "\n",
    "I'll use these charts to document some qualitative commentary on the topic models for each of the ten products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=top_ten[1]\n",
    "wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                        vanilla_final_results, \n",
    "                                        coded_final_results, \n",
    "                                        valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This product is a water enhancer - a flavor product you can add to plain water. \n",
    "\n",
    "We get the best differentiation with the vanilla inputs, though not many reviews actually matched up. There's a very generic topic that just seems to discuss trying the product, and a second topic focused on the ingredients and nutrition information of the product, especially caffeine and sugar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=top_ten[2]\n",
    "wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                        vanilla_final_results, \n",
    "                                        coded_final_results, \n",
    "                                        valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This product, which is a plastic coffee pod, also has best topic differentiation with the vanilla inputs. We get a set of reviews about the taste and flavor, and another set that seems to be all about the packaging of the product. Unfortunately, those two distinct topics don't shake out with the coded inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=top_ten[3]\n",
    "wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                        vanilla_final_results, \n",
    "                                        coded_final_results, \n",
    "                                        valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another coffee product! This is either a Starbucks coffee or one that is drawing a lot of comparisons to Starbucks coffe. Vast majority of reviews coming from a single and pretty generic topic in all three input cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=top_ten[4]\n",
    "wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                        vanilla_final_results, \n",
    "                                        coded_final_results, \n",
    "                                        valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another coffee product, this one seems to be a cappuccino product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=top_ten[5]\n",
    "wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                        vanilla_final_results, \n",
    "                                        coded_final_results, \n",
    "                                        valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a baking product, a pancake mix with the brand name Pamela's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=top_ten[6]\n",
    "wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                        vanilla_final_results, \n",
    "                                        coded_final_results, \n",
    "                                        valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another coffee product. This is another coffee pod, and in this case we again get some distinct topics for each of the input types. One topic is about the dark roast flavor and taste, and the other clearly focuses on the purchasing experience via Amazon, with words like \"purchase,\" \"order,\" \"price,\" \"shipping,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=top_ten[7]\n",
    "wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                        vanilla_final_results, \n",
    "                                        coded_final_results, \n",
    "                                        valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This product, a dog chew toy, is the first case where I'm seeing the kind of results I was really hoping for. With vanilla inputs, the reviews mostly match up into a single topic, which is also true for the valence inputs, but the coded inputs produce really nice results in two ways. \n",
    "\n",
    "First, there is a good spread of topic assignments - one major topic and two with smaller numbers of reviews. Second, the two smaller topics appear to sentiment driven: the \"BADREVIEW\" codeword stands out for Topic 6, which seems to be focusing on the durability of the product (\"crack,\" \"hard,\" \"break,\" \"last\"), and for Topic 0, although it's a little harder to tell what this topic might mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=top_ten[8]\n",
    "wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                        vanilla_final_results, \n",
    "                                        coded_final_results, \n",
    "                                        valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This product is a jar of coconut oil. In a few cases, including this one, we see that some extremely common words that didn't get filtered out are actually related to the name of the product. This is because the filtering parameters were chosen in grid optimization, and perhaps the parameters surveyed didn't allow for the exclusion of these words or maybe the coherence was higher while keeping them in (wouldn't be surprising). In future analysis tailored individually to the products, it would probably make sense to remove them. \n",
    "\n",
    "The topic differentiation for this coconut oil is best with the vanilla inputs, where there are three main topics. Topic 6 is clearly focused on the healthy attributes of the product, with mentions of \"health,\" \"healthy,\" \"benefit,\" \"organic,\" etc. Topic 3 is all about the oil as a cooking ingredient, mentioning \"smell,\" \"taste,\" \"great,\" \"cook,\" etc. Topic 0 is a little bit more of a mishmash with words including \"skin,\" \"jar,\" \"cook,\" \"order,\" \"skin,\" \"taste,\" \"smell.\" It appears to be a topic about using the oil for multiple purposes. There are a couple less-represented topics that are clearly about using the oil as a hair conditioner and skin moisturizer. \n",
    "\n",
    "Unfortunately there is nothing so interesting the results from the coded inputs - just one big topic hat may have been overwhelmed by the \"GOODREVIEW\" codeword. \n",
    "\n",
    "The valence inputs grouped reviews into two main topics, one that looks to be mostly about the product attributes (\"brand,\" \"price,\" \"jar,\" \"nutiva,\" \"organic\") and the other including terms about using it as both a cooking ingredient and a beauty product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product=top_ten[9]\n",
    "wcv.make_model_wordclouds_review_counts(product, df, \n",
    "                                        vanilla_final_results, \n",
    "                                        coded_final_results, \n",
    "                                        valence_final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This product has perhaps the best results of the bunch. The product is a dog treat called \"greenies\" for their color. The word \"greenies\" frequently appears in the wordclouds - it likely produced higher coherence among the topic when left in. \n",
    "\n",
    "With the vanilla inputs, we get three primary topics. Topic 6 clearly appears to discuss buying these treats on amazon.com vs at a pet store, and likely with a comparison of the price and the convenience. Topic 0 seems to be about the purchase experience on Amazon, and Topic 3 is focused on the effects of the treats on the teeth and breath of the dogs. \n",
    "\n",
    "With the coded topics, we seem to get slightly less variety in topics, but we do get what looks like a split between positive and negative reviews. Topic 2 has a strong positive codeword presence, and seems similar to vanilla Topic 3 about breath, teeth, and \"clean.\" Coded Topic 0 however shows more negative codewords and mentions \"time,\" \"box,\" \"size,\" \"problem,\" \"time,\" and \"minute\" - could this be complaints about the size of the treats and how long they last?\n",
    "\n",
    "Valence coded inputs also produced three main topics. Topic 0 seems to have nearly equal frequency of positive and negative codewords; not much else of interest shows in the wordcloud besides \"breath,\" \"day,\" and \"vet.\" It's hard to tell what might be the content of the other two topics, though it looks like they are generally more positive than negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram Analysis\n",
    "\n",
    "To assess topic quality and try to answer the question, \"what is this topic about?\" I will introduce a few new things, including the concept of n-grams. An n-gram is a set of n words that occur together within a document - typically NLP analyses are focused on bigrams or trigrams. We are interested in n-grams that give a picture of what is being talked about in a document. \n",
    "\n",
    "There are many different ways to determine which n-grams from a document or set of documents are the most \"important.\" One way is simply the frequency with which the n words occur together; other ways attempt to evaluate how much information the group of words is conveying when they appear together. Both could be relevant in this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = valence_topic_data.loc[(valence_topic_data['product']=='B005K4Q34S')&\n",
    "                             (valence_topic_data['review_count']>10)]\n",
    "toy.loc[toy['topic']==4]['review_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in top_ten[1:2]:\n",
    "    print('Product {}'.format(product))\n",
    "    print('Valence Coded Reviews')\n",
    "    data = valence_topic_data[valence_topic_data['product']==product]\n",
    "    for topic in data['topic'].unique():\n",
    "        if data.iloc[topic]['review_count']>10:\n",
    "            print('Topic {}'.format(topic))\n",
    "            print('Reviews: {}'.format(data.iloc[topic]['review_count']))\n",
    "            print('Topic Coherence: {}'.format(data.iloc[topic]['topic_coherence']))\n",
    "            print('Top Words: {}'.format(data.iloc[topic]['top_words']))\n",
    "            print('Best Review, fit {}: {}'.format(data.iloc[topic]['best_review_fit'], \n",
    "                                                   data.iloc[topic]['best_review']))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in top_ten[0:1]:\n",
    "    data = vanilla_topic_data[vanilla_topic_data['product']==product]\n",
    "    #y = data['topic coherence']\n",
    "    #x = data['best review fit']\n",
    "    \n",
    "    # for display purposes, take the log of the review count so the chart is visually readable\n",
    "    data['review_count_transformed'] = np.log(data['review_count'].astype(int))*10\n",
    "    \n",
    "    source = ColumnDataSource(data)\n",
    "    p = figure(x_range=(0,1.2), y_range=(0,1.2), \n",
    "               plot_height=400, plot_width=400)\n",
    "    p.circle(x='topic_coherence', y='best_review_fit', \n",
    "             size='review_count_transformed', \n",
    "             source=source, line_color='white')\n",
    "    p.yaxis.axis_label='Topic Coherence'\n",
    "    p.xaxis.axis_label='Best Review Fit'\n",
    "    \n",
    "    p.add_tools(HoverTool(tooltips=[('Topic','@topic'), \n",
    "                                    ('Reviews', '@review_count'), \n",
    "                                    ('Bigrams', '@top_bigrams_pmi'),\n",
    "                                    ('Trigrams','@top_trigrams_pmi')]))\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assign_distrib(df, encoding_type):\n",
    "    data_topics = df['{} Topic'.format(encoding_type)].value_counts()\n",
    "    data_subtopics = df['{} Subtopic'.format(encoding_type)].value_counts()\n",
    "    topics = ['Topic {}'.format(t) for t in data_topics.index]\n",
    "    subtopics = ['Subtopic {}'.format(t) for t in data_subtopics.index]\n",
    "    topic_counts = np.array(data_topics.values)\n",
    "    subtopic_counts = np.array(data_subtopics.values)\n",
    "    return topics, topic_counts, subtopics, subtopic_counts\n",
    "\n",
    "def plot_topic_distribution_with_subtopics(df, product):\n",
    "    \"\"\"\n",
    "    Plots a bar chart for each encoding type (vanilla, coded, valence coded) \n",
    "    to show the distribution of reviews assigned to each topic. \n",
    "\n",
    "    :param DataFrame df: the main dataframe containing product IDs \n",
    "                         and all review encoding sets\n",
    "    :param str product: the product for which we are plotting\n",
    "\n",
    "    :return: a bokeh gridplot object with 3 charts, one for each encoding type\n",
    "    :rtype: bokeh gridplot\n",
    "    \"\"\"\n",
    "    # get a subset of the df containing only the needed data\n",
    "    data = df[df['ProductId']==product][['ProductId', 'Vanilla Topic', \n",
    "                                         'Coded Topic', \"Valence Topic\",\n",
    "                                        'Vanilla Subtopic', 'Coded Subtopic',\n",
    "                                        'Valence Subtopic']]\n",
    "    \n",
    "    # for each encoding type, get the distribution of topic assignments and store in an array\n",
    "    \n",
    "    vanilla_topics, vanilla_counts, vanilla_subtopics, vanilla_subcounts = get_assign_distrib(data, \"Vanilla\")\n",
    "    coded_topics, coded_counts, coded_subtopics, coded_subcounts = get_assign_distrib(data, \"Coded\")\n",
    "    valence_topics, valence_counts, valence_subtopics, valence_subcounts = get_assign_distrib(data, \"Valence\")\n",
    "\n",
    "    \n",
    "    # create a CDS for each of the encoding types\n",
    "    source_vanilla = ColumnDataSource(data={'Vanilla Topics': vanilla_topics, \n",
    "                                            'Review Counts': vanilla_counts,\n",
    "                                            'Vanilla Subtopics': vanilla_subtopics,\n",
    "                                            'Subtopic Review Counts': vanilla_subcounts})\n",
    "    source_coded = ColumnDataSource(data={'Coded Topics': coded_topics, \n",
    "                                          'Review Counts': coded_counts,\n",
    "                                         'Coded Subtopics': coded_subtopics,\n",
    "                                         'Subtopic Review Counts': coded_subcounts})\n",
    "    source_valence = ColumnDataSource(data={'Valence Topics': valence_topics, \n",
    "                                             'Review Counts': valence_counts,\n",
    "                                           'Valence Subtopics': valence_subtopics,\n",
    "                                           'Subtopic Review Counts': valence_subcounts})\n",
    "\n",
    "\n",
    "    \n",
    "    # create a figure for each encoding type showing the distribution of review assignments\n",
    "    l = figure(x_range=vanilla_topics, plot_height=300, y_range=(0,550), \n",
    "               plot_width=325, toolbar_location=None, \n",
    "               title='Vanilla Topic Distribution for Product {}'.format(product))\n",
    "    l.vbar(x=dodge('Vanilla Topics', 0.0, range=l.x_range), \n",
    "           top='Review Counts', width=0.7, source=source_vanilla)\n",
    "    l.vbar(x=dodge('Vanilla Subtopics', 0.25, range=l.x_range), \n",
    "           top='Subtopic Review Counts', width=0.7, source=source_vanilla)\n",
    "    l.title.text_font_size='9pt'\n",
    "    l.xaxis.major_label_orientation = 0.75\n",
    "    \n",
    "    c = figure(x_range=coded_topics, plot_height=300, y_range=(0,550), \n",
    "              plot_width=325, toolbar_location=None, \n",
    "              title='Coded Topic Distibution for Product {}'.format(product))\n",
    "    c.vbar(x='Coded Topics', top='Review Counts', width=0.9, source=source_coded)\n",
    "    c.title.text_font_size='9pt'\n",
    "    c.xaxis.major_label_orientation = 0.75\n",
    "\n",
    "    \n",
    "    r = figure(x_range=valence_topics, plot_height=300, y_range=(0,550), \n",
    "              plot_width=325, toolbar_location=None, \n",
    "              title='Valence Topic Distibution for Product {}'.format(product))\n",
    "    r.vbar(x='Valence Topics', top=\"Review Counts\", width=0.9, source=source_valence)\n",
    "    r.title.text_font_size='9pt'\n",
    "    r.xaxis.major_label_orientation = 0.75\n",
    "\n",
    "    # lay the three charts out in a grid\n",
    "    grid = gridplot([[l, c, r]])\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in top_ten[0:1]:\n",
    "    grid = plot_topic_distribution_with_subtopics(df, product)\n",
    "    show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assign_distrib(df, encoding_type):\n",
    "    data_topics = df['{} Topic'.format(encoding_type)].value_counts()\n",
    "    data_subtopics = df['{} Subtopic'.format(encoding_type)].value_counts()\n",
    "    topics = ['Topic {}'.format(t) for t in data_topics.index]\n",
    "    subtopics = ['Subtopic {}'.format(t) for t in data_subtopics.index]\n",
    "    topic_counts = np.array(data_topics.values)\n",
    "    subtopic_counts = np.array(data_subtopics.values)\n",
    "    return data_topics, data_subtopics, topics, topic_counts, subtopics, subtopic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
