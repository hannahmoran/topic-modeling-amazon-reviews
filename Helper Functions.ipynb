{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim, logging, io, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "handler = logging.FileHandler('training_output.log')\n",
    "\n",
    "\n",
    "# The handler above is somthing I needed with respect to logging.\n",
    "# Gensim performs various calculations while training the LDA model that I am using, but the only way to see them\n",
    "# is in the logging outputs.\n",
    "# Specifically, I need to capture the perplexity values during training to verify that perplexity is decreasing.\n",
    "# This metric is needed to compare models and to do hyperparameter tuning. \n",
    "\n",
    "\n",
    "# The following blog post was helpful to me in figure out how to make the log handler I needed.\n",
    "# https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the LDA model, I'd like to filter out the top 3 most frequently used tokens in the corpus\n",
    "# For most products, the most frequent tokens  refer to the product itself in a generic way and also contain what we might consider stop words\n",
    "# but didn't want to remove earlier because they are needed for part-of-speech tagging\n",
    "# (e.g., \"product,\" \"coconut,\" \"oil\" as well as \"have,\" \"be,\" etc.)\n",
    "# this helper function is a slight modification of one of gensim's built-in methods -\n",
    "# I don't want to remove the codewords I've inserted, and those also end up in the top 3 quite often\n",
    "\n",
    "def remove_freq(dictionary, n):\n",
    "    logger = logging.getLogger('gensim.corpora.dictionary')\n",
    "    save = set(['GOODREVIEW', 'BADREVIEW', 'VGOODREVIEW', 'VBADREVIEW'])\n",
    "    most_frequent_ids = (v for v in (dictionary.token2id).values() if dictionary[v] not in save)\n",
    "    most_frequent_ids = sorted(most_frequent_ids, key=dictionary.dfs.get, reverse=True)\n",
    "    most_frequent_ids = most_frequent_ids[:n]\n",
    "    # do the actual filtering, then rebuild dictionary to remove gaps in ids\n",
    "    most_frequent_words = [(dictionary[idx], dictionary.dfs.get(idx, 0)) for idx in most_frequent_ids]\n",
    "    logger.info(\"discarding %i tokens: %s...\", len(most_frequent_ids), most_frequent_words[:10])\n",
    "\n",
    "    dictionary.filter_tokens(bad_ids=most_frequent_ids)\n",
    "    logger.info(\"resulting dictionary: %s\", dictionary)\n",
    "\n",
    "# the next few helper functions deal with extracting metrics of interest from gensim's logger, \n",
    "# which are being dumped into a log file as training runs\n",
    "# I am capturing bounds, perplexity, and per-word topic differences\n",
    "    \n",
    "    \n",
    "import itertools\n",
    "# thanks to these SO answers https://stackoverflow.com/questions/6213063/python-read-next\n",
    "# and https://stackoverflow.com/questions/5434891/iterate-a-list-as-pair-current-next-in-python\n",
    "# for showing a way to deal with lines of a file in groups of three\n",
    "def threes(iterator):\n",
    "    \"s -> (s0,s1,s2), (s1,s2,s3), (s2, s3, s4), ...\"\n",
    "    a, b, c = itertools.tee(iterator, 3)\n",
    "    next(b, None)\n",
    "    next(c, None)\n",
    "    next(c, None)\n",
    "    return zip(a, b, c)\n",
    "\n",
    "def capture_logs(): # capture the perplexity, per-word bound, and topic difference values from the logger and save\n",
    "    perplexity_log = []\n",
    "    perplex = {}\n",
    "    bounds = {}\n",
    "    diff = {}\n",
    "    with open(\"training_output.log\", 'r') as f:\n",
    "        for line in f:\n",
    "            if re.match(\"|\".join([r'.*topic diff.*', r'.*per-word.*', r'.*PROGRESS.*']), line):\n",
    "                perplexity_log.append(line)\n",
    "    for a, b, c in threes(perplexity_log): \n",
    "        if re.match(r'.*PROGRESS.*', a):\n",
    "            pass_val = int(re.search(r'\\d*, at', a).group(0).split(',')[0])\n",
    "            if re.match(r'.*topic diff.*', b):\n",
    "                d = float(re.search(r'\\d*\\.\\d*', b).group(0).split()[0])\n",
    "                diff[pass_val] = d\n",
    "            if re.match(r'.*per-word.*', b): # these may show up in the second line of the group as well\n",
    "                b = float(re.search(r'.\\d*\\.\\d* per', b).group(0).split()[0])\n",
    "                p = float(re.search(r'\\d*\\.\\d perplexity', b).group(0).split()[0])\n",
    "                perplex[pass_val] = p\n",
    "                bounds[pass_val] = b\n",
    "            if re.match(r'.*per-word.*', c):\n",
    "                b = float(re.search(r'.\\d*\\.\\d* per', c).group(0).split()[0])\n",
    "                p = float(re.search(r'\\d*\\.\\d perplexity', c).group(0).split()[0])\n",
    "                perplex[pass_val] = p\n",
    "                bounds[pass_val] = b\n",
    "    return bounds, perplex, diff\n",
    "\n",
    "def perplexity_decreasing(perplex): #checks if the perplexity decreased during the last two training passes\n",
    "    passes = sorted(perplex.keys())\n",
    "    start = passes[-2]\n",
    "    end = passes[-1]\n",
    "    if perplex[start] > perplex[end]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function runs trains an LDA model for a single product, \n",
    "# constructing any number of topics over any number of passes\n",
    "def run_lda(product, n_topics, n_passes, texts, save_path):\n",
    "    \"\"\"\n",
    "    This function trains an LDA model fo a single product, \n",
    "    constructing any number of topics over any number of training passes.\n",
    "    product: the string product ID\n",
    "    n_topics: number of topics desired\n",
    "    n_passes: number of training passes to make\n",
    "    texts: the corpus to use (column name from the main dataframe)\n",
    "    save_path: where to save the model outputs\n",
    "    returns a dataframe with the results of the training \n",
    "    \"\"\"\n",
    "    os.remove('training_output.log')\n",
    "    logger = logging.getLogger('gensim.models.ldamodel')\n",
    "    handler = logging.FileHandler('training_output.log')\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s : %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    data = df[df['ProductId']==product]\n",
    "    texts = data[texts].str.split()\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    remove_freq(dictionary, 3)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "    mm = corpora.MmCorpus('/tmp/corpus.mm')\n",
    "    chunk_size = review_counts[product]/3\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=dictionary, num_topics=n_topics, update_every=1, chunksize=chunk_size, passes=n_passes)\n",
    "    bounds, perplex, diff = capture_logs()\n",
    "    results = pd.DataFrame(index=[product], data={'num_topics': n_topics, 'chunk': chunk_size, 'passes': n_passes})\n",
    "    results['per-word bounds'] = [bounds]\n",
    "    results['perplexity'] = [perplex]\n",
    "    results['topic diff'] = [diff]\n",
    "    p = sorted(perplex.keys())\n",
    "    end = p[-1]\n",
    "    results['final perplexity'] = perplex[end]\n",
    "    d = sorted(diff.keys())\n",
    "    end = p[-1]\n",
    "    results['final topic diff'] = diff[end]\n",
    "    if perplexity_decreasing:\n",
    "        results['perplexity decreasing'] = True\n",
    "    else: \n",
    "        results['perplexity_decreasing'] = False\n",
    "    for n in range(0,n_topics):\n",
    "        topic = lda.show_topic(n, 20)\n",
    "        results['topic {}'.format(n)] = [topic]\n",
    "    lda.save('./{}/{}'.format(save_path, product))\n",
    "    lda.clear()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function tunes an LDA model for a single product, by grid searching over various numbers of topics,\n",
    "# over different numbers of passes, using a specified text set (cleaned, coded, valence coded, etc.)\n",
    "from shutil import copyfile\n",
    "from sys import exit\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "def tune_lda(product, n_topics, n_passes, input_text, save_path):\n",
    "    os.remove('training_output.log')\n",
    "    logger = logging.getLogger('gensim.models.ldamodel')\n",
    "    handler = logging.FileHandler('training_output.log')\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s : %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    data = df[df['ProductId']==product]\n",
    "    texts = data[input_text].str.split()\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    remove_freq(dictionary, 10)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "    mm = corpora.MmCorpus('/tmp/corpus.mm')\n",
    "    chunk_size = review_counts[product]/3\n",
    "    output = pd.DataFrame(columns=['product', 'num_topics', 'chunk', 'passes', 'per-word bounds', 'perplexity', 'topic diff',\n",
    "                                  'final perplexity', 'final topic diff', 'perplexity decreasing', 'coherence'])\n",
    "    for t in n_topics:\n",
    "        for p in n_passes:\n",
    "            print('training LDA with {} topics over {} passes'.format(t, p))\n",
    "            lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=dictionary, \\\n",
    "                                                  num_topics=t, update_every=1, \\\n",
    "                                                  chunksize=chunk_size, passes=p, random_state=42)\n",
    "            bounds, perplex, diff = capture_logs()\n",
    "            results = {'product': product, 'num_topics': t, 'chunk': chunk_size, 'passes': p}\n",
    "            results['per-word bounds'] = [bounds]\n",
    "            results['perplexity'] = [perplex]\n",
    "            results['topic diff'] = [diff]\n",
    "            per = sorted(perplex.keys())\n",
    "            end = per[-1]\n",
    "            results['final perplexity'] = perplex[end]\n",
    "            d = sorted(diff.keys())\n",
    "            end = d[-1]\n",
    "            results['final topic diff'] = diff[end]\n",
    "            if perplexity_decreasing:\n",
    "                results['perplexity decreasing'] = True\n",
    "            else: \n",
    "                results['perplexity_decreasing'] = False\n",
    "            for n in range(0,t):\n",
    "                topic = lda.show_topic(n, 20)\n",
    "                results['topic {}'.format(n)] = [topic]\n",
    "            lda.save('./{}/{}_{}_{}'.format(save_path, product, t, p))\n",
    "            cm = CoherenceModel(model=lda, corpus=corpus, texts = texts, coherence='c_v')\n",
    "            results['coherence'] = cm.get_coherence()\n",
    "            output = pd.concat([output, pd.DataFrame(data=results)], axis=0)\n",
    "            lda.clear()\n",
    "    return output\n",
    "\n",
    "# this function finds the best result (maximizing coherence) from the grid search run\n",
    "# the results are saved to final output dataframe and the LDA model is also saved off\n",
    "\n",
    "def save_best(output, final_results, save_path):\n",
    "    output.reset_index(inplace=True)\n",
    "    best_idx = output['coherence'].idxmax()\n",
    "    product = output.loc[best_idx, 'product']\n",
    "    print('best results for product {}:'.format(product))\n",
    "    print(output.loc[best_idx])\n",
    "    final_results = final_results.append(output.loc[best_idx], ignore_index=True)\n",
    "    t = output.loc[best_idx, 'num_topics']\n",
    "    p = output.loc[best_idx, 'passes']\n",
    "    lda = gensim.models.ldamodel.LdaModel.load(\"./{}/{}_{}_{}\".format(save_path, product, t, p))\n",
    "    lda.save('./{}/final_models/{}_{}_{}'.format(save_path, product, t, p))\n",
    "    lda.clear()\n",
    "    del output\n",
    "    print(\"Final model saved for product {} with {} topics over {} passes.\".format(product, t, p))\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
